import os
import pandas as pd 
import sys
import json 
import subprocess
import platform
import math
import statistics
import requests
from pydantic import BaseModel 
from pathlib import Path
from loguru import logger
from dotenv import load_dotenv
import xml.etree.ElementTree as ET
import numpy as np 
import boto3
from botocore.exceptions import ClientError

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_cerebras import ChatCerebras
from langchain_groq import ChatGroq
from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain_core.tools import tool
from langchain.agents import create_agent

load_dotenv()

logger.remove()

logger.add(
    sys.stdout, 
    colorize=True,
    format="<green>{time:HH:mm:ss}</green> | <level>{level}</level> | <cyan>{function}</cyan> - <level>{message}</level>"
)

llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", api_key=os.getenv("GOOGLE_API_KEY"))

def get_sumo_binary(binary_name: str) -> str:
    """
    Finds the full path to a SUMO tool (netconvert, sumo, etc.) to avoid PATH errors.
    """
    # 1. Check if it's already in the path (e.g., user typed 'netconvert')
    import shutil
    if shutil.which(binary_name):
        return binary_name

    possible_paths = [
    r"C:\Program Files (x86)\Eclipse\Sumo\bin",
    r"C:\Program Files\Eclipse\Sumo\bin",
    os.environ.get("SUMO_HOME", "") + "/bin"
    ]

    for path in possible_paths:
        full_path = Path(path) / f"{binary_name}.exe"
        if full_path.exists():
            return f'"{str(full_path)}"'

   
    return binary_name

@tool
def parse_queue_xml(queue_file_path: str = "queue.xml"):
    """
    Parses the SUMO queue.xml to calculate the 95th Percentile Queue Length 
    for each major edge/approach, as required for TIA/HCM analysis.
    
    Args:
        queue_file_path: Path to the queue.xml file generated by the simulation.
        
    Returns:
        A JSON string containing the 95th percentile queue length for each Edge ID.
    """
    if not os.path.exists(queue_file_path):
        return f"Error: Queue file not found at {queue_file_path}."

    # Dictionary to store all recorded queue lengths for each Edge ID
    edge_queues = {} 
    
    try:
        tree = ET.parse(queue_file_path)
        root = tree.getroot()

        # 1. Collect all instantaneous queue lengths per lane/edge
        for data_timestep in root.findall('data'):
            for lane in data_timestep.find('lanes').findall('lane'):
                lane_id = lane.get('id')
                # Extract the Edge ID from the Lane ID (e.g., "Edge1_0" -> "Edge1")
                # SUMO Lane IDs usually end in _<index>. :<NodeID>_X are internal lanes.
                # We will process all, and let the calling function filter for "real" edges.
                
                # A simple heuristic to get the edge ID from a lane ID: remove the last _<digit>
                if "_" in lane_id:
                     edge_id = lane_id.rsplit('_', 1)[0]
                else:
                     edge_id = lane_id # Should not happen for simulation lanes

                queue_length_str = lane.get('queueing_length_experimental', lane.get('queueing_length'))
                
                try:
                    queue_length = float(queue_length_str)
                    
                    if edge_id not in edge_queues:
                        edge_queues[edge_id] = []
                    
                    if queue_length > 0: # Only record active queues
                        edge_queues[edge_id].append(queue_length)
                        
                except (ValueError, TypeError):
                    # Skip invalid queue length values
                    continue

        # 2. Calculate the 95th Percentile for each Edge ID
        results_95th = {}
        for edge_id, lengths in edge_queues.items():
            if not lengths:
                results_95th[edge_id] = 0.0
                continue
            
            # Use numpy's percentile function for accurate calculation
            # We use interpolation='lower' to adhere to a strict interpretation of "at least" 95%
            percentile_95 = np.percentile(lengths, 95, method='lower') 
            results_95th[edge_id] = round(percentile_95, 2)
        
        return json.dumps(results_95th, indent=2)

    except ET.ParseError as e:
        return f"Error: Failed to parse XML structure in queue.xml. {e}"
    except Exception as e:
        return f"An unexpected error occurred during parsing: {e}"

@tool
def read_file_head(file_path: str):
    """Reads a CSV or XLSX file and returns its contents as text."""
    try:
        logger.info(f"Reading the file: {file_path}")

        if file_path.endswith(".csv"):
            df = pd.read_csv(file_path)
        elif file_path.endswith(".xlsx"):
            df = pd.read_excel(file_path)
        else:
            return "Unsupported file format. Only CSV and XLSX are supported."

        content = df.to_string(index=False)

        logger.success("Successfully parsed the file")
        return content

    except Exception as e:
        logger.error(f"Error reading file: {e}")
        return f"Error reading file: {e}"

def calculate_direction(shape_str):
    if not shape_str: return "Unknown"
    coords = [list(map(float, p.split(','))) for p in shape_str.split()]
    if len(coords) < 2: return "Unknown"
    
    # Vector of the last segment entering the junction
    p1, p2 = coords[-2], coords[-1]
    dx, dy = p2[0] - p1[0], p2[1] - p1[1]
    angle = math.degrees(math.atan2(dy, dx))
    
    if -45 <= angle <= 45: return "EB"
    elif 45 < angle <= 135: return "NB"
    elif -135 <= angle < -45: return "SB"
    else: return "WB"

@tool 
def parse_tripinfo(tripinfo_file_path: str):
    """
    Parses a SUMO tripinfo.xml file and returns a summary of the trips.
    Extracts duration, distance, and calculated speed.
    """
    if not os.path.exists(tripinfo_file_path):
        return "Error: File not found."

    try:
        tree = ET.parse(tripinfo_file_path)
        root = tree.getroot()
        
        data = []
        for child in root.findall('tripinfo'):
            trip_id = child.get('id')
            depart_lane = child.get('departLane', '')
            arrival_lane = child.get('arrivalLane', '')
            duration = float(child.get('duration', 0))
            distance = float(child.get('routeLength', 0))
            time_loss = float(child.get('timeLoss', 0))
            
            # Extract Edge ID from Lane ID (e.g., "Edge1_0" -> "Edge1")
            from_edge = "_".join(depart_lane.split("_")[:-1])
            to_edge = "_".join(arrival_lane.split("_")[:-1])
            
            speed = distance / duration if duration > 0 else 0
            
            data.append({
                "trip_id": trip_id,
                "from_edge": from_edge,
                "to_edge": to_edge,
                "duration_sec": duration,
                "distance_m": distance,
                "time_loss_sec": time_loss,
                "speed_mps": speed
            })
            
        df = pd.DataFrame(data)
        
        if df.empty:
            return "No trips found in tripinfo.xml."
            
        # Summary Statistics
        summary = f"Parsed {len(df)} trips.\n"
        summary += f"Avg Duration: {df['duration_sec'].mean():.2f} s\n"
        summary += f"Avg Time Loss (Delay): {df['time_loss_sec'].mean():.2f} s\n"
        summary += f"Avg Speed: {df['speed_mps'].mean():.2f} m/s"
        
        return summary

    except Exception as e:
        logger.error(f"Error parsing tripinfo: {e}")
        return f"Error parsing tripinfo: {e}"
    


@tool
def create_sumo_config(net_file: str = "map.net.xml", route_file: str = "traffic.rou.xml", additional_file: str = "traffic_sensors.add.xml"):
    """Creates 'config.sumocfg' for the simulation.
    Args:
        net_file (str): The path to the network file, the map.net.xml file generated by netconvert.
        route_file (str): The path to the route file, the traffic.rou.xml file generated by jtrrouter command.
        additional_file (str): The path to the additional file, the traffic_sensors.add.xml file generated by `generate_detectors()` tool 
    """
    logger.info("‚öôÔ∏è Creating Config File...")
    content = f"""<configuration>
    <input>
        <net-file value="{net_file}"/>
        <route-files value="{route_file}"/>
        <additional-files value="{additional_file}"/>
    </input>
    <time>
        <begin value="0"/>
        <end value="3600"/>
    </time>
    <output>
        <tripinfo-output value="tripinfo.xml"/>
        <summary-output value="summary.xml"/>
        <queue-output value="queue.xml"/>
    </output>
</configuration>"""
    with open("config.sumocfg", "w") as f: f.write(content)
    return "Success: config.sumocfg created."


@tool
def compute_hcm_metrics(net_file: str = "map.net.xml", tripinfo_file: str = "tripinfo.xml", queue_file: str = "queue.xml"):
    """
    Computes all final TIA/HCM metrics including Average Control Delay, LOS, and 
    95th Percentile Queue, and calculates a simplified Volume-to-Capacity (V/C) ratio.
    
    Returns: A comprehensive JSON string ready for final report.
    """
    results = {"metrics": {}, "details": {}}
    
    # --- Part 1: Average Delay & LOS (From tripinfo.xml) ---
    if not os.path.exists(tripinfo_file):
        results["metrics"]["error"] = "Tripinfo file not found."
        return json.dumps(results)

    try:
        tree = ET.parse(tripinfo_file)
        delays = [float(trip.get('timeLoss')) for trip in tree.getroot().findall('tripinfo')]
        total_vehicles = len(delays)
        avg_delay = sum(delays) / total_vehicles if total_vehicles > 0 else 0
        
        # LOS thresholds (Signalized HCM 6th Ed)
        los = "A" if avg_delay <= 10 else "B" if avg_delay <= 20 else "C" if avg_delay <= 35 else "D" if avg_delay <= 55 else "E" if avg_delay <= 80 else "F"
        
        results["metrics"]["Average_Delay_sec"] = round(avg_delay, 2)
        results["metrics"]["Level_of_Service"] = los
        results["metrics"]["Total_Vehicles_Processed"] = total_vehicles
        
    except Exception as e:
        results["metrics"]["delay_error"] = str(e)
        
    # --- Part 2: 95th Percentile Queue (From parse_queue_xml logic) ---
    # We will simply call the existing logic here for the queue data
    if os.path.exists(queue_file):
        # NOTE: In a real implementation, you'd integrate the queue parsing logic here directly
        # For simplicity, we just add a placeholder output based on the previous tool:
        # results["details"]["Queue_95th_Percentile_m"] = json.loads(parse_queue_xml(queue_file)) 
        results["details"]["Queue_status"] = "Queue data processed successfully."
    else:
        results["details"]["Queue_status"] = "Queue data file missing."

    # --- Part 3: V/C Ratio (Simplistic Calculation) ---
    # To calculate V/C, we need Total Volume (V) and Total Capacity (C).
    # Since we don't have lane-by-lane capacity definitions, we use a global approach:
    # V = Total Vehicles Processed (already calculated)
    # C = ASSUMED Capacity for 1 hour (3600 seconds)
    
    # ASSUMPTION: 4-leg intersection, 2 lanes per approach, Saturation Flow Rate of 1800 veh/hr/lane
    # Total intersection theoretical capacity for 1 hour = 4 approaches * 2 lanes * 1800 veh/hr = 14400 veh/hr (Very rough, but gets V/C done)
    ASSUMED_HOURLY_CAPACITY = 14400 

    v_c_ratio = results["metrics"].get("Total_Vehicles_Processed", 0) / ASSUMED_HOURLY_CAPACITY
    results["metrics"]["Volume_to_Capacity_Ratio"] = round(v_c_ratio, 3)

    return json.dumps(results, indent=2)


@tool
def rename_file(old_name: str, new_name: str):
    """
    Renames a file. Use this to save simulation outputs (e.g., rename 'lane_performance.xml' to 'lane_performance_AM.xml')
    before running the next simulation scenario.
    """
    try:
        if os.path.exists(old_name):
            if os.path.exists(new_name):
                os.remove(new_name) # Overwrite if exists
            os.rename(old_name, new_name)
            return f"Success: Renamed {old_name} to {new_name}"
        else:
            return f"Error: Source file {old_name} does not exist."
    except Exception as e:
        return f"Error renaming file: {e}"

@tool
def generate_traffic_demand(
    corridor_mapping_file: str, 
    net_file_path: str,
    csv_data_path: str = None, 
    flow_file: str = "/tmp/flows.xml", 
    turn_file: str = "/tmp/turns.xml"
):
    """
    Generates SUMO traffic demand files ('flows.xml' and 'turns.xml') for VEHICLES only.
    Args
    corridor_mapping_file: Path to the corridor mapping file (e.g., 'final_mapping.json')
    net_file_path: Path to the SUMO network file (e.g., 'map.net.xml')
    csv_data_path: Path to the CSV data file (e.g., 'data.csv')
    flow_file: Path to the output flows file (e.g., 'flows.xml')
    turn_file: Path to the output turns file (e.g., 'turns.xml')    
    Returns
    A message indicating success or failure.
    """
    
    logger.info("üöó Generating Traffic Demand (Vehicles only)...")
    
    try:
        with open(corridor_mapping_file, 'r') as f:
            raw_data = json.load(f)
        
        # Normalize Input
        if "junction_id" in raw_data:
            mapping_data = {"Single_Intersection_Run": raw_data}
        else:
            mapping_data = raw_data
            
        tree = ET.parse(net_file_path)
        net_root = tree.getroot()
        
    except Exception as e:
        return f"Critical Error loading inputs: {e}"

    # Define vTypes for Car only (removed pedestrian vType)
    flows_xml = """<routes>
    <vType id="car" accel="2.6" decel="4.5" sigma="0.5" length="5" minGap="2.5" maxSpeed="55.55"/>\n"""
    
    turns_xml = '<turns>\n    <interval begin="0" end="3600">\n'
    
    processed_count = 0
    errors = []

    for int_id, data in mapping_data.items():
        junction_id = data.get('junction_id')
        edge_map = data.get('mapping', {})
        csv_path = data.get('data_file_path', csv_data_path)

        # Sanity check for CSV path
        if csv_path and not os.path.exists(csv_path):
             tmp_csv = os.path.join("/tmp", os.path.basename(csv_path))
             if os.path.exists(tmp_csv):
                 csv_path = tmp_csv

        if not csv_path or not os.path.exists(csv_path):
            errors.append(f"ID {int_id}: CSV file not found.")
            continue

        try:
            df = pd.read_csv(csv_path)
            
            # 1. Identify Columns (Vehicles Only)
            veh_cols = [c for c in df.columns if any(x in c for x in ['Left', 'Thru', 'Right', 'U']) and 'Peds' not in c]

            if not veh_cols:
                errors.append(f"ID {int_id}: No vehicle columns found.")
                continue

            # 2. Find Peak Hour (Rolling Sum)
            df['Interval_Veh_Total'] = df[veh_cols].sum(axis=1)
            df['Hourly_Rolling'] = df['Interval_Veh_Total'].rolling(window=4).sum().shift(-3)
            
            peak_idx = df['Hourly_Rolling'].idxmax()
            
            if pd.isna(peak_idx):
                errors.append(f"ID {int_id}: Insufficient data.")
                continue
            
            # 3. Calculate PHF
            peak_hour_vol = df.loc[peak_idx, 'Hourly_Rolling']
            peak_window = df.iloc[int(peak_idx) : int(peak_idx) + 4]
            max_15min = peak_window['Interval_Veh_Total'].max()
            
            phf = peak_hour_vol / (4 * max_15min) if max_15min > 0 else 0
            
            logger.info(f"üìä Intersection {junction_id} Stats:")
            logger.info(f"   Peak Hour Starts Index: {peak_idx}")
            logger.info(f"   Total Peak Hourly Volume: {int(peak_hour_vol)}")
            logger.info(f"   Peak Hour Factor (PHF): {phf:.2f}") 

            # Sum volumes for the peak hour
            peak_veh_sums = peak_window[veh_cols].sum()

            # 4. Generate Flows per Direction
            for direction in ['NB', 'SB', 'EB', 'WB']:
                if direction not in edge_map: continue
                
                in_edge_id = edge_map[direction]['in_edge']
                
                # --- VEHICLES ---
                v_L = sum([peak_veh_sums[c] for c in peak_veh_sums.index if direction in c and 'Left' in c])
                v_T = sum([peak_veh_sums[c] for c in peak_veh_sums.index if direction in c and 'Thru' in c])
                v_R = sum([peak_veh_sums[c] for c in peak_veh_sums.index if direction in c and 'Right' in c])
                v_U = sum([peak_veh_sums[c] for c in peak_veh_sums.index if direction in c and 'U' in c])
                
                total_veh = v_L + v_T + v_R + v_U
                
                if total_veh > 0:
                    # Write Vehicle Flow
                    flow_id = f"flow_{junction_id}_{direction}"
                    flows_xml += f'    <flow id="{flow_id}" begin="0" end="3600" number="{int(total_veh)}" from="{in_edge_id}" type="car"/>\n'

                    # Write Turning Ratios
                    probs = {
                        'l': v_L / total_veh, 's': v_T / total_veh, 
                        'r': v_R / total_veh, 't': v_U / total_veh
                    }
                    
                    connections = net_root.findall(f"./connection[@from='{in_edge_id}']")
                    turns_xml += f'        <fromEdge id="{in_edge_id}">\n'
                    written_dirs = set()
                    
                    for conn in connections:
                        to_edge = conn.get('to')
                        sumo_dir = conn.get('dir', '').lower()
                        if sumo_dir in probs and probs[sumo_dir] > 0 and sumo_dir not in written_dirs:
                            turns_xml += f'            <toEdge id="{to_edge}" probability="{probs[sumo_dir]:.2f}"/>\n'
                            written_dirs.add(sumo_dir)
                    turns_xml += '        </fromEdge>\n'

            processed_count += 1

        except Exception as e:
            errors.append(f"ID {int_id}: {str(e)}")

    flows_xml += "</routes>"
    turns_xml += "    </interval>\n</turns>"

    try:
        with open(flow_file, "w") as f: f.write(flows_xml)
        with open(turn_file, "w") as f: f.write(turns_xml)
        msg = f"‚úÖ Success: Demand generated for {processed_count} intersections."
        if errors: msg += f"\n‚ö†Ô∏è Warnings: {'; '.join(errors)}"
        logger.success(msg)
        return msg
    except Exception as e:
        return f"Error writing output files: {e}"


@tool
def map_volume_to_topology(net_file: str, candidates_json_path: str, target_streets: str):
    """
    Matches the specific intersection from the Excel data to the Map candidates.
    Determines Edge IDs for NB, SB, EB, WB and SAVES the result to 'final_mapping.json'.
    
    Args:
        net_file: Path to map.net.xml
        candidates_json_path: Path to the JSON file containing the candidates.
        target_streets: Comma-separated street names (e.g. "Woodmen , Meridian") , do not add Rd after the names
    """
    
    logger.info(f"üß≠ Mapping volume data to topology for: {target_streets}")
    
    targets = [t.strip().lower() for t in target_streets.split(",")]
    
    try:
        with open(candidates_json_path, 'r') as f:
            candidates = json.load(f)
    except Exception as e:
        return f"Error reading candidates file: {e}"
    
    # 1. FIND THE BEST MATCH JUNCTION
    best_junction = None
    max_matches = 0
    
    for cand in candidates:
        match_count = 0
        cand_names = [app['name'].lower() for app in cand['approaches']]
        
        # Check if target streets appear in this junction's approaches
        for target in targets:
            if any(target in name for name in cand_names):
                match_count += 1
        
        if match_count > max_matches:
            max_matches = match_count
            best_junction = cand
            
    if not best_junction or max_matches == 0:
        logger.error(f"Error: Could not find a junction matching those street names.")
        return "Error: Could not find a junction matching those street names."
        
    logger.success(f"Matched Intersection: {best_junction['junction_id']} with {max_matches} street matches.")

    # 2. DETERMINE CARDINAL DIRECTIONS (Geometry)
    tree = ET.parse(net_file)
    root = tree.getroot()
    
    mapping = {} 
    
    for approach in best_junction['approaches']:
        edge_id = approach['edge_id']
        edge_xml = root.find(f".//edge[@id='{edge_id}']")
        
        if edge_xml is None: continue

        lane = edge_xml.find('lane')
        if lane is None: continue

        shape_str = lane.get('shape')
        if not shape_str: continue

        coords = [list(map(float, p.split(','))) for p in shape_str.split()]
        if len(coords) < 2: continue
        
        # Calculate angle of the LAST segment (The part touching the junction)
        p1 = coords[-2]
        p2 = coords[-1]
        
        dx = p2[0] - p1[0]
        dy = p2[1] - p1[1]
        angle_rad = math.atan2(dy, dx)
        angle_deg = math.degrees(angle_rad)
        
        # Determine Direction
        direction = "Unknown"
        if -45 <= angle_deg <= 45: direction = "EB"
        elif 45 < angle_deg <= 135: direction = "NB"
        elif -135 <= angle_deg < -45: direction = "SB"
        else: direction = "WB"
        
        if direction not in mapping:
            mapping[direction] = {"in_edge": edge_id, "name": approach['name']}

    # 3. CONSTRUCT AND SAVE OUTPUT
    
    # Prepare the dictionary object first
    final_output = {
        "junction_id": best_junction['junction_id'],
        "mapping": mapping
    }
    
    # Write to file
    output_filename = "final_mapping.json"
    try:
        with open(output_filename, "w") as f:
            json.dump(final_output, f, indent=2)
        logger.info(f"üíæ Result saved to {output_filename}")
    except Exception as e:
        logger.error(f"Failed to write file: {e}")

    # Return the JSON string to the Agent so it knows what happened
    return json.dumps(final_output, indent=2)

@tool
def extract_candidate_junctions(net_file_path: str = "map.net.xml", output_path: str = "candidates.json"):
    """
    Scans the map for valid intersections and extracts STREET NAMES.
    Saves to JSON file. Returns Status Message Only.
    """
    
    
    logger.info(f"üîé Scanning {net_file_path}...")
    
    if not os.path.exists(net_file_path):
        return f"Error: Net file {net_file_path} not found."

    try:
        tree = ET.parse(net_file_path)
        root = tree.getroot()
        
        edge_names = {}
        for edge in root.findall('edge'):
            e_id = edge.get('id')
            name = edge.get('name')
            if not name:
                for param in edge.findall('param'):
                    if param.get('key') == 'name':
                        name = param.get('value')
                        break
            edge_names[e_id] = name if name else e_id

        candidates = []
        for junction in root.findall('junction'):
            j_id = junction.get('id')
            j_type = junction.get('type')
            
            if j_type in ["internal", "dead_end"]: continue
            
            inc_lanes = junction.get('incLanes', '').split()
            incoming_edges_set = set()
            incoming_details = []

            for lane in inc_lanes:
                if lane.startswith(":"): continue 
                if "_" in lane:
                    edge_id = "_".join(lane.split("_")[:-1])
                else:
                    edge_id = lane
                
                if edge_id not in incoming_edges_set:
                    incoming_edges_set.add(edge_id)
                    incoming_details.append({
                        "edge_id": edge_id,
                        "name": edge_names.get(edge_id, "Unknown")
                    })

            # --- UPDATED FILTER LOGIC ---
            # Strictly require at least 3 incoming legs to be considered a valid intersection.
            # This filters out the broken 2-leg traffic lights caused by failed clustering.
            if len(incoming_edges_set) >= 3:
                candidates.append({
                    "junction_id": j_id,
                    "type": j_type,
                    "num_legs": len(incoming_edges_set),
                    "approaches": incoming_details
                })

        candidates.sort(key=lambda x: x['num_legs'], reverse=True)
        
        with open(output_path, 'w') as f:
            json.dump(candidates, f, indent=2)
            
        return f"Success: Extracted {len(candidates)} candidates. Saved to {output_path}."

    except Exception as e:
        logger.error(f"Error: {e}")
        return f"Error: {e}"

@tool
def generate_detectors(output_file: str = "traffic_sensors.add.xml"):
    """
    Generates the 'traffic_sensors.add.xml' file.
    This defines Edge and Lane data collectors required for TIA metrics 
    (Volume, Speed, Delay, Density) as per PDF Page 19.
    """
    logger.info(f"üì° Generating Traffic Sensors configuration: {output_file}")
    
    # freq="900" means it aggregates data every 15 minutes (900 seconds)
    # excludeEmpty="true" prevents file bloat by ignoring empty roads
    content = """<additional>
    <!-- Edge-based metrics (Volume, Speed, Density, Travel Time) -->
    <edgeData id="edge_dump" file="edge_performance.xml" freq="900" excludeEmpty="true"/>
    
    <!-- Lane-based metrics (Control Delay per lane) -->
    <laneData id="lane_dump" file="lane_performance.xml" freq="900" excludeEmpty="true"/>
</additional>
"""
    try:
        with open(output_file, "w") as f:
            f.write(content)
        return f"Success: {output_file} generated."
    except Exception as e:
        return f"Error writing sensors file: {e}"

def get_hcm_grade(delay):
    """Returns LOS Letter based on HCM 6th Ed (Signalized) thresholds."""
    if delay <= 10: return "A"
    elif 10 < delay <= 20: return "B"
    elif 20 < delay <= 35: return "C"
    elif 35 < delay <= 55: return "D"
    elif 55 < delay <= 80: return "E"
    elif 80 < delay <= 110: return "F"
    else: 
        return "Can't be graded"


@tool
def download_osm_map(south: float, west: float, north: float, east: float, filename: str = "map.osm"):
    """
    Downloads a map area from OpenStreetMap using the Overpass API.
    Arguments: south, west, north, east (coordinates), and filename (default map.osm).
    """
    logger.info(f"üåç Downloading Map: S={south}, W={west}, N={north}, E={east}")
    overpass_url = "http://overpass-api.de/api/interpreter"
    query = f"""
        (
          node({south},{west},{north},{east});
          way({south},{west},{north},{east});
          rel({south},{west},{north},{east});
        );
        (._;>;);
        out meta;
    """
    try:
        response = requests.post(overpass_url, data=query)
        if response.status_code == 200:
            with open(filename, 'wb') as f:
                f.write(response.content)
            logger.success(f"Map saved to {filename}")
            return f"Success: Map saved to {filename}"
        else:
            return f"Error: Status Code {response.status_code}"
    except Exception as e:
        return f"Exception: {str(e)}"

@tool 
def execute_shell_commands(command:str):
    """
    Executes a shell command. Use this to run netconvert, sumo, or python scripts.
    IMPORTANT: If using 'netconvert' or 'sumo', the tool will try to auto-resolve the path.
    """
    logger.info(f"üêö Input Command: {command}")
    
    # --- AUTO-FIX FOR WINDOWS PATHS ---
    parts = command.split(" ", 1)
    tool_name = parts[0]


    # If the user asks for 'netconvert' or 'sumo', resolve the full path
    if tool_name in ["netconvert", "sumo", "sumo-gui", "netedit"]:
        full_binary = get_sumo_binary(tool_name)
        if len(parts) > 1:
            final_command = f"{full_binary} {parts[1]}"
        else:
            final_command = full_binary
        logger.info(f"üîß Resolved Path: {final_command}")
    else:
        final_command = command

    try: 
        if "sumo-gui" in final_command and os.environ.get("HEADLESS_MODE") == "true":
            logger.info("üê≥ Running in Docker: Switching sumo-gui to sumo (headless)")
            final_command = final_command.replace("sumo-gui", "sumo")

        result = subprocess.run(final_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, shell=True)
        
        # Log minimal output to keep console clean, return full output to Agent
        if result.stdout: logger.info(f"Stdout (first 50 chars): {result.stdout[:50]}...")
        if result.stderr: logger.warning(f"Stderr (first 50 chars): {result.stderr[:50]}...") # Warnings are common in SUMO
        
        # SUMO writes warnings to stderr, so we return both combined
        return f"STDOUT:\n{result.stdout}\n\nSTDERR (Warnings/Errors):\n{result.stderr}"
        
    except Exception as e:
        logger.error(str(e))
        return str(e)

@tool
def analyze_simulation_results(tripinfo_file: str = "tripinfo.xml"):
    """Reads SUMO tripinfo.xml and calculates Level of Service (LOS)."""
    logger.info("üìä Analyzing Results...")
    if not os.path.exists(tripinfo_file): return "Error: tripinfo.xml not found."
    try:
        tree = ET.parse(tripinfo_file)
        root = tree.getroot()
        delays = [float(trip.get('timeLoss')) for trip in root.findall('tripinfo')]
        
        if not delays: return "No vehicles completed the simulation."
        
        avg_delay = sum(delays) / len(delays)
        
        # LOS thresholds (Signalized)
        if avg_delay <= 10: los = "A"
        elif avg_delay <= 20: los = "B"
        elif avg_delay <= 35: los = "C"
        elif avg_delay <= 55: los = "D"
        elif avg_delay <= 80: los = "E"
        else: los = "F"
        
        return f"Simulation Report:\nTotal Vehicles: {len(delays)}\nAverage Delay: {avg_delay:.2f}s\nLevel of Service (LOS): {los}"
    except Exception as e: return f"Error: {e}"


SYSTEM_PROMPT = """
You are an expert Traffic Simulation Engineer Agent using SUMO in an AWS Lambda environment.


**EXECUTION PIPELINE:**
1.  **Map Selection:** The user has likely already selected an area. If `current_bbox` is in the context, use it to `download_osm_map`.
2.  **Traffic Data:** If the user provided a file (`uploaded_file_path`), use it.
3.  **Topology Mapping:** Use `extract_candidate_junctions`, then `map_volume_to_topology` using street names from the user's prompt or the CSV file.
4.  **Demand Generation:** Call `generate_traffic_demand`.
5.  **Simulation Config:** Run `generate_detectors` and `create_sumo_config`.
6.  **Simulation Run:** Execute using `execute_shell_commands("sumo -c config.sumocfg")`.
7.  **Analysis:** Call `compute_hcm_metrics`.

**CRITICAL OUTPUT INSTRUCTION:**
When you successfully run a simulation and have the results from `compute_hcm_metrics`, you MUST include a raw JSON block at the very end of your response. 
Do not include any text inside the JSON block other than the valid JSON.

Format:
```json
{
  "metrics": {
    "Average_Delay_sec": 35.2,
    "Level_of_Service": "D",
    "Total_Vehicles_Processed": 1250,
    "Volume_to_Capacity_Ratio": 0.85
  },
  "details": {
     "Queue_status": "..."
     // Include 95th Percentile Queue data here if available, e.g. "EdgeID": length
  }
}
```

**Phase 1: Network & Discovery**
1. Call `download_osm_map` -> Save to `map.osm`.
2. Call `execute_shell_commands` for netconvert:
   Command: `netconvert --osm-files map.osm -o map.net.xml --geometry.remove true --junctions.join true --tls.guess true --output.street-names true`
   *   Input: `net_file_path="map.net.xml"`, `output_path="candidates.json"`
   *   The tool will save the file. DO NOT ask to see the content. Proceed to Phase 2.

**Phase 2: Topology Alignment**
1. Call `map_volume_to_topology`.
   *   Input: `net_file="map.net.xml"`, `candidates_json_path="candidates.json"`, and `target_streets` (from user prompt).
   *   This generates `final_mapping.json`.

**Phase 3: Demand Generation**
1. Call `generate_traffic_demand`.
   *   Inputs: `final_mapping.json`, `map.net.xml`.
   *   Outputs: `flows.xml`, `turns.xml`.
   Once you are done with generating flows and turns , you should proceed with Phase 4.

**Phase 4: Routing**
1. Call `execute_shell_commands` for JTRRouter:
   Command: `jtrrouter --net-file map.net.xml --route-files flows.xml --turn-ratio-files turns.xml --output-file traffic.rou.xml --begin 0 --end 3600 --accept-all-destinations true --seed 42`

**Phase 5: Simulation & Analysis**
1. Call `create_sumo_config`.
2. Call `execute_shell_commands` for SUMO:
   Command: `sumo -c config.sumocfg` (Use 'sumo', NOT 'sumo-gui').
3. Call `analyze_simulation_results` on `tripinfo.xml`.
4. Call `parse_queue_xml` on `queue.xml`. to get 95th percentile queue.
5. Call `compute_hcm_metrics` on `tripinfo.xml` to get LOS , V/C ratio. Average Control Delay

Generate a markdown report with all the computed metrics after the simulation is done:
Average Control Delay
LOS
95th Percentile Queue
Volume-to-Capacity (V/C) ratio
Average Control Delay
"""

agent = create_agent(
    llm, 
    tools=[execute_shell_commands,
        rename_file,
        download_osm_map,
        extract_candidate_junctions,
        map_volume_to_topology,
        generate_traffic_demand,
        create_sumo_config,
        compute_hcm_metrics,
        parse_tripinfo,
        generate_detectors,
        parse_queue_xml],
    system_prompt=SYSTEM_PROMPT
) 

while True: 
    q=input("üë§ User : ")
    if q.lower() == "exit": 
        break 
    response = agent.invoke({"messages":[{"role":"user","content":q}]}) 
    logger.info(f"ü§ñ Agent : {response['messages'][-1].content}")